{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO CNNS\n",
    "\n",
    "Teneis que hacer un modelo de ___convulsiones___. El dataset lo teneis aqui:\n",
    "\n",
    "https://www.kaggle.com/c/cifar-10/data\n",
    "\n",
    "IMPORTANTE: No useis el test.7z. Descargaos solo el train.7z y las train_labels. Despues ya dividis en train-test y con ese test ya sacais las metricas finales.\n",
    "\n",
    "QUE OS PIDO:\n",
    "- Descargad el dataset\n",
    "- Leedlo en el NB (podeis investigar como hacerlo o usad el NB de landscapes como punto de partida)\n",
    "- Haced train-test y escalar si procede\n",
    "- Montad un modelo que tenga: Al menos 3 bloques convolucionales (Conv2D+Pooling+Dropout). PERO TIENE QUE SER UN MODELO FUNCIONAL (https://keras.io/guides/functional_api/). Es una chorrada, pero teneis que investigar la sintaxis nueva.\n",
    "- Clasificacion, 10 clases, 0.1 de validation split... ya sabeis.\n",
    "- Entrenad con los parametros que querais e id jugando con el numero de epocas, dropout, o incluso mas o menos capas, numero de filtros... etc... (lo hemos visto todo en clase)\n",
    "- Evaluad el modelo con las metricas que creais convenientes\n",
    "\n",
    "\n",
    "NOTAS: \n",
    "- Las imagenes son de 32x32\n",
    "- Son a color luego el shape ya sabeis...\n",
    "- Recordad que la primera capa debe llevar el input_shape\n",
    "- Si haceis una red muy grande puede ir lento, asi que empezad despacito y creced si procede\n",
    "- Usad la historia de epocas (train_loss vs val_loss) para ver si estais overfiteando\n",
    "- No es imprescindible pero muy muy muy recomendable que hagais el ejercicio en un google colab. Tendreis que ver como subir los datos etc.. pero las epocas iran muuucho mas rapidas y asi aprendeis. Podeis hacerlo local, ver que funciona y migrar a colab, o directamente hacerlo todo en colab\n",
    "- En teoria, salvo que hay que declarar el modelo Functional, todo lo hemos visto ya. Intentad probar cosas e intentad no abusar de Chatgpt. Daos la oportunidad de hacerlo vosotros y demostraros que podeis. \n",
    "\n",
    "- Ah! El ejercicio es OBLIGATORIO. Teneis 3 dias y una tarde para hacerlo, no hay excusas. Todos los aprendizajes de hacer este NB los voy a dar por sabidos. No lo corregiré en clase, pero resolveré todas las dudas que traigais, eso si.\n",
    "\n",
    "Si durante el finde os surgen dudas me escribis. Pero me gustaria que intentarais resolverlo con la maxima autonomia. Pero escribidme si os atascais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "\n",
    "The CIFAR-10 data consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images in the official data. We have preserved the train/test split from the original dataset. \n",
    "\n",
    " The provided files are:\n",
    "\n",
    "- train.7z - a folder containing the training images in png format\n",
    "- trainLabels.csv - the training labels\n",
    "\n",
    "The label classes in the dataset are:\n",
    "\n",
    "- airplane \n",
    "- automobile \n",
    "- bird \n",
    "- cat \n",
    "- deer \n",
    "- dog \n",
    "- frog \n",
    "- horse \n",
    "- ship \n",
    "- truck\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_CSV = \"trainLabels.csv\"\n",
    "IMAGE_SIZE = (32,32)\n",
    "TRAIN_PATH = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frog': 0,\n",
       " 'truck': 1,\n",
       " 'deer': 2,\n",
       " 'automobile': 3,\n",
       " 'bird': 4,\n",
       " 'horse': 5,\n",
       " 'ship': 6,\n",
       " 'cat': 7,\n",
       " 'dog': 8,\n",
       " 'airplane': 9}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv(\"trainLabels.csv\")\n",
    "class_names = (names[\"label\"].unique()).tolist()\n",
    "class_names_labels = {class_names:i for i, class_names in enumerate(class_names)}\n",
    "class_names_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, im_size, labels_csv):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Leer etiquetas del archivo CSV\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    \n",
    "    # Crear el diccionario de etiquetas\n",
    "    labels_dict = {f\"{row['id']}.png\": row['label'] for _, row in labels_df.iterrows()}\n",
    "    \n",
    "    # Filtrar solo los archivos presentes en el directorio\n",
    "    valid_files = set(os.listdir(path))\n",
    "    labels_dict = {k: v for k, v in labels_dict.items() if k in valid_files}\n",
    "    \n",
    "    print(f\"Total de archivos a procesar: {len(valid_files)}\")\n",
    "\n",
    "    # Procesar imágenes con un contador\n",
    "    for idx, file in enumerate(valid_files):\n",
    "        image_path = os.path.join(path, file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, im_size)\n",
    "        X.append(image)\n",
    "        y.append(labels_dict[file])  # Asignar etiqueta según el archivo CSV\n",
    "        \n",
    "        # Mostrar progreso cada 1000 imágenes\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Procesadas {idx + 1} imágenes...\")\n",
    "\n",
    "    print(\"Carga completada.\")\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos a procesar: 50000\n",
      "Procesadas 1000 imágenes...\n",
      "Procesadas 2000 imágenes...\n",
      "Procesadas 3000 imágenes...\n",
      "Procesadas 4000 imágenes...\n",
      "Procesadas 5000 imágenes...\n",
      "Procesadas 6000 imágenes...\n",
      "Procesadas 7000 imágenes...\n",
      "Procesadas 8000 imágenes...\n",
      "Procesadas 9000 imágenes...\n",
      "Procesadas 10000 imágenes...\n",
      "Procesadas 11000 imágenes...\n",
      "Procesadas 12000 imágenes...\n",
      "Procesadas 13000 imágenes...\n",
      "Procesadas 14000 imágenes...\n",
      "Procesadas 15000 imágenes...\n",
      "Procesadas 16000 imágenes...\n",
      "Procesadas 17000 imágenes...\n",
      "Procesadas 18000 imágenes...\n",
      "Procesadas 19000 imágenes...\n",
      "Procesadas 20000 imágenes...\n",
      "Procesadas 21000 imágenes...\n",
      "Procesadas 22000 imágenes...\n",
      "Procesadas 23000 imágenes...\n",
      "Procesadas 24000 imágenes...\n",
      "Procesadas 25000 imágenes...\n",
      "Procesadas 26000 imágenes...\n",
      "Procesadas 27000 imágenes...\n",
      "Procesadas 28000 imágenes...\n",
      "Procesadas 29000 imágenes...\n",
      "Procesadas 30000 imágenes...\n",
      "Procesadas 31000 imágenes...\n",
      "Procesadas 32000 imágenes...\n",
      "Procesadas 33000 imágenes...\n",
      "Procesadas 34000 imágenes...\n",
      "Procesadas 35000 imágenes...\n",
      "Procesadas 36000 imágenes...\n",
      "Procesadas 37000 imágenes...\n",
      "Procesadas 38000 imágenes...\n",
      "Procesadas 39000 imágenes...\n",
      "Procesadas 40000 imágenes...\n",
      "Procesadas 41000 imágenes...\n",
      "Procesadas 42000 imágenes...\n",
      "Procesadas 43000 imágenes...\n",
      "Procesadas 44000 imágenes...\n",
      "Procesadas 45000 imágenes...\n",
      "Procesadas 46000 imágenes...\n",
      "Procesadas 47000 imágenes...\n",
      "Procesadas 48000 imágenes...\n",
      "Procesadas 49000 imágenes...\n",
      "Procesadas 50000 imágenes...\n",
      "Carga completada.\n"
     ]
    }
   ],
   "source": [
    "X, y = read_data(TRAIN_PATH, IMAGE_SIZE, LABELS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos las etiquetas a índices\n",
    "y = np.array([class_names_labels[label] for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las imagenes\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 32, 32, 3)\n",
      "y_train shape: (40000,)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la capa de entrada\n",
    "# CIFAR-10 tiene imágenes de 32x32 con 3 canales de color (RGB)\n",
    "input_layer = Input(shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bloques convolucionales**\n",
    "\n",
    "1. La capa convolucinal (Conv2D) es la capa fundamental de una CNN. Esta capa aplica filtros(kernels) sobre la imagen de entrada para extraer características como bordes, formas y texturas. La convolución hace que el modelo aplique estos filtros sobre la imagen, realizando una multiplicación con los valores de la imagen, sumándolos para crear un mapa de activación. Cada filtro se entrena para aprender características específicas de los datos y esti permite a la red capturar patrones espaciales.\n",
    "```python\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "```\n",
    "\n",
    "- **32** es el número de filtros que se aplican a la imagen y cada filtro tiene un tamaño de **(3,3)** y genera un mapa de activación que captura diferentes patrones.\n",
    "- **activation = 'relu'** es la función que se aplica después de la convolución, esto introduce no linealidad en el modelo ayudando a aprender patrones más complejos.\n",
    "- **padding = 'same'** asegura que las dimensiones de salida sean las mismas que las de entrada. Si no lo usamos la imagen reduciría su tamaño.\n",
    "\n",
    "2. La capa de pooling (MaxPooling2D) se utiliza para reducir las dimensiones espaciales(alto y ancho) de las imágenes, sin perder información importante. Esto se logra mediante la selección de los valores más altos de un conjunto de píxeles vecinos. Es importante porque reduce la complejidad computacional y ayuda a que el modelo sea menos sensible a pequeñas traslaciones y desplazamientos en las imagenes(invarianza a pequeñas transformaciones).\n",
    "```python\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "```\n",
    "\n",
    "- **(2, 2)** es el tamaño de la ventana de pooling, significa que en este caso tomamos el valor máximo de cada bloque de 2x2 píxeles. La salida de esta operación trendrá la mitad de tamaño en cada dimensión (alto y ancho) en comparación con la entrada.\n",
    "\n",
    "3. La capa dropout (Dropout) es una técnica de regularización que desactiva aleatoriamente un porcentaje de las neuronas durante el entrenamiento. Esto ayuda a prevenir el overfitting, ya que evita que el modelo dependa demasiado de neuronas espacíficas. Es importante para que el modelo generalice bien.\n",
    "```python\n",
    "x = Dropout(0.25)(x)\n",
    "```\n",
    "\n",
    "- **0.25** esto significa que el 25% de las neuronas de desactivarán aleatoriamente durante cada paso del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer bloque convolucional\n",
    "```python\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)  # 33, 33, 32 (dimensión)\n",
    "```\n",
    "\n",
    "\n",
    "Conv2D(32, (3, 3), activation='relu', padding='same'): Aquí se define una capa convolucional 2D. Los parámetros de esta capa son:\n",
    "\n",
    "32: El número de filtros o kernels que se aplicarán a la entrada. Cada filtro aprende características diferentes de la imagen.\n",
    "(3, 3): El tamaño del filtro es de 3x3 píxeles.\n",
    "activation='relu': Se aplica la función de activación ReLU (Rectified Linear Unit), que transforma los valores negativos a cero y deja los positivos sin cambio. Esto introduce no linealidad en el modelo.\n",
    "\n",
    "padding='same': Significa que se aplicará un \"relleno\" (padding) para mantener las dimensiones de la imagen de entrada constantes después de la convolución. Es decir, si la imagen de entrada tiene dimensiones H×W×C (alto, ancho, canales), la salida tendrá las mismas dimensiones en altura y anchura, solo que con un número de canales diferente (32 en este caso).\n",
    "\n",
    "Resultado: Si la entrada tenía dimensiones (33, 33, 3) (por ejemplo, una imagen RGB de 33x33 píxeles), la salida tendrá dimensiones (33, 33, 32). La salida de esta capa tendrá 32 canales (uno por cada filtro).\n",
    "\n",
    "```python\n",
    "x = MaxPooling2D((2,2))(x) #16, 16, 32\n",
    "```\n",
    "\n",
    "MaxPooling2D((2, 2)): Esta capa realiza una operación de \"max pooling\", que reduce las dimensiones espaciales de la imagen (alto y ancho). En este caso, se toma una ventana de tamaño 2x2 y, de cada ventana, se selecciona el valor máximo.\n",
    "\n",
    "Esto reduce la resolución espacial de la imagen, ayudando a extraer características más robustas y reduciendo la cantidad de parámetros.\n",
    "Al aplicar el max pooling sobre una entrada de tamaño (33, 33, 32), la salida tendrá dimensiones (16, 16, 32), ya que el max pooling reduce a la mitad las dimensiones espaciales (33/2 ≈ 16).\n",
    "\n",
    "Resultado: La salida tiene dimensiones (16, 16, 32), es decir, se ha reducido el tamaño en la dirección de alto y ancho, pero el número de canales sigue siendo 32.\n",
    "\n",
    "```python\n",
    "x = Dropout(0.25)(x)\n",
    "```\n",
    "\n",
    "Dropout(0.25): El \"dropout\" es una técnica de regularización que ayuda a prevenir el sobreajuste (overfitting) durante el entrenamiento. Durante el entrenamiento, se \"apagan\" aleatoriamente el 25% de las unidades (neuronas) en la capa. Esto significa que el 25% de los valores en la salida de esta capa se establecerán a cero en cada paso de entrenamiento, lo que fuerza a la red a no depender demasiado de ningún conjunto específico de neuronas.\n",
    "\n",
    "Este valor de 0.25 significa que el 25% de las conexiones de esta capa serán eliminadas aleatoriamente.\n",
    "Resultado: La salida sigue teniendo las mismas dimensiones que antes de aplicar dropout (16, 16, 32), pero algunas de las activaciones se habrán puesto a cero de manera aleatoria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer bloque convolucional\n",
    "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_layer) #33, 33, 32 (dimensión)\n",
    "x = MaxPooling2D((2,2))(x) #16, 16, 32\n",
    "x = Dropout(0.25)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo bloque concolucional \n",
    "x = Conv2D(64, (3,3), activation='relu', padding='same')(x) #16, 16, 64\n",
    "x = MaxPooling2D((2,2))(x) #8, 8 64\n",
    "x = Dropout(0.25)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tercer bloque convolucional\n",
    "x = Conv2D(128, (3,3), activation='relu', padding='same')(x) #8, 8, 128\n",
    "x = MaxPooling2D((2,2))(x) #4, 4, 128\n",
    "x = Dropout(0.25)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las capas convolucionales generan datos 3D, antes de pasar a las capas densas aplanamos los datos\n",
    "x = Flatten()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Capas densas y capa de salida**\n",
    "\n",
    "### **Capas densas**\n",
    "\n",
    "Después de las capas convolucionales, el modelo ha aprendido patrones locales de la imagen como bordes, texturas o combinaciones más complejas. Sin embargo, estos patrones deben combinarse para tomar decisiones sobre la clase imagen. \n",
    "\n",
    "La capa densa conecta todas las neurones de la capa anterior con cada neurona de la capa densa. Esto se llama una \"capa totalmente conectada\" (fully connected layer).\n",
    "\n",
    "En este caso, después de pasar por las capas convolucionales, tenemos una representación de la imagen en una forma reducida (por el pooling y a la activación de la convolucion), pero todavía en 3D. Necesitamos aplanar esta representación para convertirla en un vector 1D antes de poder pasarlo a una capa densa. Esto se hace mediante la operación **Flatten()**. Después de aplanar la capa densa ayuda a realizar las decisiones finales sobre la clasificación de la imagen. \n",
    "\n",
    "La capa densa tiene una gran capacidad de aprendizaje no lineal. Como cada neurona está conectada a todas las neuronas de la capa anterior, la capa densa puede combinar todas las características extraídas por las capas convolucionales para tomar decisiones complejas sobre la clase de la imagen.\n",
    "```python\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "```\n",
    "\n",
    "- **Flatten()** toma las salidas de las capas convolucionales (3D) y las aplana en un solo vector de valores. Por ejemplo, si la salida de las capas anteriores tiene una forma  (batch_size, 4, 4, 128), al aplicar Flatten() la forma se convierte en (batch_size, 2048).\n",
    "- **Dense(128)** significa que la capa tiene 128 neuronas y cada una de ellas está conectada a todas las neuronas de la capa anterior (el vector aplanado). **activation='relu'** aplica una función de activación ReLU.\n",
    "\n",
    "### **Capa de salida**\n",
    "\n",
    "Después de pasar por la capa densa, necesitamos una capa final para clasificar la imagen en una de las 10 categorías de CIFAR-10. Esto lo realiza la capa de salida.\n",
    "\n",
    "La capa de salida es una capa densa con tantas neuronas como clases de salida haya (en este caso 10), y utiliza la función de activación **softmax**. Esta función convierte las salidas de la capa densa en probabilidades, la clase con probabilidad más alta será la que se prediga como la clase de la imagen.\n",
    "\n",
    "Esta capa es importante porque decide a qué clase pertenece la imagen.\n",
    "```python\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "```\n",
    "\n",
    "- **Dense(10)** significa que hay 10 neuronas, una por cada clase en CIFAR-10 (de 0 a 9). Esta capa decide en qué clase encaja mejor la imagen.\n",
    "- **activation='softmax'** es la función que convierte las salidas de las neuronas en probabilidades. La clase con la probabilidad más alta será la predicción final del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos capas densas\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "output_layer = Dense(10, activation='softmax')(x) # CIFAR-10 tiene 10 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creamos el modelo**\n",
    "\n",
    "Cuando creamos el modelo en Keras, estamos construyendo toda la arquitectura de la red neuronal, es decir, estamos definiendo las capas que componen el modelo, cómo se conectan entre ellas y qué tipo de operaciones realizan. Este es el proceso de configurar el flujo de los datos que atraviesa las capas, desde la entrada (las imágenes) hasta la salida (las predicciones de las clases).\n",
    "\n",
    "En este caso,  la creación del modelo se hace a través del modelo funcional. Lo que hacemos es definir las entradas, aplicar las capas que hemos configurado y, finalmente, obtener la salida que predice la clase de la imagen.\n",
    "\n",
    "### **Qué significa el model.summary()**\n",
    "\n",
    "El resumen que devuelve este comando proporciona una visión general de la arquitectura de la red neuronal que acabamos de construir. Este resumen es muy útil para entender cómo está configurada la red y verificar que todo está en orden. También es importante para comprobar el número de parámetros entrenables de cada capa.\n",
    "\n",
    "1. **Capas Convolucionales (Conv2D)** en cada capa convolucional los parámetros que se entrenan son los pesos de los filtros y los sesgos.\n",
    "```python\n",
    "Parámetros en Conv2D=(f×f×canales de entrada+1)×número de filtros\n",
    "```\n",
    "- En el caso de la primera capa convolucional: tamaño del filtro = 3x3; canales de entrada = 3 (por ser una imagen RGB); número de filtros = 32; parámetros por filtro = (3x3x3+1) = 28; total de parámetros = 32.\n",
    "\n",
    "2. **Capas de MaxPooling (MaxPooling2D)** no tienen parámetros entrenables, su función es reducir la dimensión espacial (alto y ancho) de las imágenes, pero no agregan pesos ni sesgos.\n",
    "\n",
    "3. **Capas de Dropout (Dropout)** tampoco tienen parámetros, su objetivo es prevenir el overfitting.\n",
    "\n",
    "4. **Capa Densa (Dense)** conectan todas las neuronas de la capa anterior con cada neurona de la capa siguiente. Los parámetros que se entrenan son los pesos de las conexiones y los sesgos.\n",
    "```python \n",
    "Parámetros en Dense=(número de neuronas de la capa anterior+1)×número de neuronas de la capa densa\n",
    "```\n",
    "\n",
    "- Neuronas de la capa anterior = 2048 (salida de la capa Flatten); neuronas en la caoa densa = 128; parámetros = (2048 +1)x128 =262272.\n",
    "\n",
    "5. **Capa de salida** es también una capa densa pero con 10 neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "model = Model(inputs=input_layer, outputs=output_layer, name=\"CIFAR10_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CIFAR10_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CIFAR10_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Revisamos la estructura del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos el modelo\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "563/563 - 27s - 48ms/step - accuracy: 0.7921 - loss: 0.5843 - val_accuracy: 0.7800 - val_loss: 0.6751\n",
      "Epoch 2/2\n",
      "563/563 - 25s - 45ms/step - accuracy: 0.7988 - loss: 0.5695 - val_accuracy: 0.7755 - val_loss: 0.6789\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "history = model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - 7ms/step - accuracy: 0.7682 - loss: 0.6559\n",
      "Precisión en test: 0.77\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Precisión en test: {test_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
